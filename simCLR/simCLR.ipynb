{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1682321946468,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"Ch0ofG7WKcX9","outputId":"a8b37560-5ea2-4dda-d96f-a6e006212641"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682321946469,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"MCZ8b6cHKtmT","outputId":"db05bb82-e4ee-44b6-9384-a3afb9c1ddfa"},"outputs":[],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":577,"status":"ok","timestamp":1682321947042,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"LkejbbwRKxPl","outputId":"240a1786-0082-434a-dbd8-aadb2d430b50"},"outputs":[],"source":["!mkdir /content/saved_models"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8146,"status":"ok","timestamp":1682321955186,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"NQSm-_9PK0C4"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import shutil, time, os, requests, random, copy\n","from itertools import permutations \n","import seaborn as sns\n","import imageio\n","from skimage.transform import rotate, AffineTransform, warp, resize\n","#import skvideo.io as vidio\n","#from google.colab.patches import cv2_imshow\n","from IPython.display import clear_output, Image, SVG\n","import h5py\n","#from tabulate import tabulate\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as tF\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.datasets as dset\n","from torchvision import datasets, transforms, models\n","#from torchviz import make_dot\n","#from torchsummary import summary\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, f1_score\n","from sklearn.metrics import average_precision_score, precision_recall_curve, roc_auc_score, accuracy_score\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","#import matplotlib.animation as animation\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682321955188,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"tZ7CnwZsK4pr"},"outputs":[],"source":["# from torchvision.models.utils import load_state_dict_from_url\n","from typing import Type, Any, Callable, Union, List, Optional\n","from torch import Tensor\n","from collections import OrderedDict "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682321955188,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"VUd-ynfLLD3H","outputId":"4e5bcf18-567b-451d-ff8c-8611cacdc137"},"outputs":[],"source":["np.random.seed(16)\n","torch.manual_seed(16)\n","#tf.random.set_seed(16)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":482,"status":"ok","timestamp":1682321955665,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"7vVLuciMLFsV"},"outputs":[],"source":["\n","from sklearn.manifold import TSNE"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682321955666,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"QFVBXIh3LG6v"},"outputs":[],"source":["def set_seed(seed = 16):\n","    np.random.seed(16)\n","    torch.manual_seed(16)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3003,"status":"ok","timestamp":1682321958664,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"id9HUyBjLIAQ","outputId":"6b2758fa-2c96-4431-daf8-1295fac86e1a"},"outputs":[],"source":["!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2706,"status":"ok","timestamp":1682321961368,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"9lC9Dv-ELMND"},"outputs":[],"source":["# !tar -xf data/cifar-10-python.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682321961369,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"3GLOmqtGLNTE"},"outputs":[],"source":["import pickle\n","def unpickle(file):\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":552,"status":"ok","timestamp":1682321961918,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"h70huCy-LOc-","outputId":"ebc7f30d-633e-41ba-fd2c-6a399f60573a"},"outputs":[],"source":["train_files = ['data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5']\n","images = np.array([],dtype=np.uint8).reshape((0,3072))\n","labels = np.array([])\n","for tf in train_files:\n","    data_dict = unpickle('data/cifar-10-batches-py/'+tf)\n","    data = data_dict[b'data']\n","    images = np.append(images,data,axis=0)\n","    labels = np.append(labels,data_dict[b'labels'])\n","print(images.shape, labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682321961918,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"tMLbuI8BLPvP","outputId":"9ac0e0ab-583b-433d-c111-a692f57ffba5"},"outputs":[],"source":["testimages = np.array([],dtype=np.uint8).reshape((0,3072))\n","testlabels = np.array([])\n","\n","data_dict = unpickle('data/cifar-10-batches-py/test_batch')\n","data = data_dict[b'data']\n","testimages = np.append(testimages,data,axis=0)\n","testlabels = np.append(testlabels,data_dict[b'labels'])\n","print(testimages.shape, testlabels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682321961919,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"Pa61XchDLQm5","outputId":"5b43d499-9663-4289-b1fd-dfdbd71537b7"},"outputs":[],"source":["images = images.reshape((-1,3,32,32))\n","testimages = testimages.reshape((-1,3,32,32))\n","\n","print(images.shape, testimages.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1375,"status":"ok","timestamp":1682321963292,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"PUXej9uvLRii","outputId":"6c0a1049-18eb-494d-daf2-92f4b6d30f41"},"outputs":[],"source":["images = images.astype(np.float)\n","testimages = testimages.astype(np.float)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1682321963292,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"2HewS1qiLVWC"},"outputs":[],"source":["trimages = np.moveaxis(images,1,3)\n","tsimages = np.moveaxis(testimages,1,3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1682321963292,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"k0Ib2rleLW0e","outputId":"0d84c536-2844-4e1e-c03d-b2a968c08fc9"},"outputs":[],"source":["labels = labels.astype(np.int)\n","testlabels = testlabels.astype(np.int)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682321963293,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"hEvakAvpLX8e"},"outputs":[],"source":["lab_dict = {0:'airplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',6:'frog',7:'horse',8:'ship',9:'truck'}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682321963293,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"m_miKZxHLY0I","outputId":"ecd0dbcb-fa70-4ed0-837b-518062ec98dc"},"outputs":[],"source":["plt.imshow(trimages[1]/255.0)\n","plt.show()\n","print(lab_dict[labels[1]])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682321963293,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"_wBb9HRgLcyT"},"outputs":[],"source":["#util_wk2\n","def TP(y, pred, th=0.5):\n","    pred_t = (pred > th)\n","    return np.sum((pred_t == True) & (y == 1))\n","\n","\n","def TN(y, pred, th=0.5):\n","    pred_t = (pred > th)\n","    return np.sum((pred_t == False) & (y == 0))\n","\n","\n","def FN(y, pred, th=0.5):\n","    pred_t = (pred > th)\n","    return np.sum((pred_t == False) & (y == 1))\n","\n","def FP(y, pred, th=0.5):\n","    pred_t = (pred > th)\n","    return np.sum((pred_t == True) & (y == 0))\n","\n","def get_accuracy(y, pred, th=0.5):\n","    tp = TP(y,pred,th)\n","    fp = FP(y,pred,th)\n","    tn = TN(y,pred,th)\n","    fn = FN(y,pred,th)\n","    \n","    return (tp+tn)/(tp+fp+tn+fn)\n","\n","def get_prevalence(y):\n","    return np.sum(y)/y.shape[0]\n","\n","def sensitivity(y, pred, th=0.5):\n","    tp = TP(y,pred,th)\n","    fn = FN(y,pred,th)\n","    \n","    return tp/(tp+fn)\n","\n","def specificity(y, pred, th=0.5):\n","    tn = TN(y,pred,th)\n","    fp = FP(y,pred,th)\n","    \n","    return tn/(tn+fp)\n","\n","def get_ppv(y, pred, th=0.5):\n","    tp = TP(y,pred,th)\n","    fp = FP(y,pred,th)\n","    \n","    return tp/(tp+fp)\n","\n","def get_npv(y, pred, th=0.5):\n","    tn = TN(y,pred,th)\n","    fn = FN(y,pred,th)\n","    \n","    return tn/(tn+fn)\n","\n","\n","def get_performance_metrics(y, pred, class_labels, tp=TP,\n","                            tn=TN, fp=FP,\n","                            fn=FN,\n","                            acc=get_accuracy, prevalence=get_prevalence, \n","                            spec=specificity,sens=sensitivity, ppv=get_ppv, \n","                            npv=get_npv, auc=roc_auc_score, f1=f1_score,\n","                            thresholds=[]):\n","    if len(thresholds) != len(class_labels):\n","        thresholds = [.5] * len(class_labels)\n","\n","    columns = [\"Injury\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"Prevalence\",\n","               \"Sensitivity\",\n","               \"Specificity\", \"PPV\", \"NPV\", \"AUC\", \"F1\", \"Threshold\"]\n","    df = pd.DataFrame(columns=columns)\n","    for i in range(len(class_labels)):\n","        df.loc[i] = [class_labels[i],\n","                     round(tp(y[:, i], pred[:, i]),3),\n","                     round(tn(y[:, i], pred[:, i]),3),\n","                     round(fp(y[:, i], pred[:, i]),3),\n","                     round(fn(y[:, i], pred[:, i]),3),\n","                     round(acc(y[:, i], pred[:, i], thresholds[i]),3),\n","                     round(prevalence(y[:, i]),3),\n","                     round(sens(y[:, i], pred[:, i], thresholds[i]),3),\n","                     round(spec(y[:, i], pred[:, i], thresholds[i]),3),\n","                     round(ppv(y[:, i], pred[:, i], thresholds[i]),3),\n","                     round(npv(y[:, i], pred[:, i], thresholds[i]),3),\n","                     round(auc(y[:, i], pred[:, i]),3),\n","                     round(f1(y[:, i], pred[:, i] > thresholds[i]),3),\n","                     round(thresholds[i], 3)]\n","\n","    df = df.set_index(\"Injury\")\n","    return df\n","\n","def bootstrap_metric(y, pred, classes, metric='auc',bootstraps = 100, fold_size = 1000):\n","    statistics = np.zeros((len(classes), bootstraps))\n","    if metric=='AUC':\n","        metric_func = roc_auc_score\n","    if metric=='Sensitivity':\n","        metric_func = sensitivity\n","    if metric=='Specificity':\n","        metric_func = specificity\n","    if metric=='Accuracy':\n","        metric_func = get_accuracy\n","    for c in range(len(classes)):\n","        df = pd.DataFrame(columns=['y', 'pred'])\n","        df.loc[:, 'y'] = y[:, c]\n","        df.loc[:, 'pred'] = pred[:, c]\n","        # get positive examples for stratified sampling\n","        df_pos = df[df.y == 1]\n","        df_neg = df[df.y == 0]\n","        prevalence = len(df_pos) / len(df)\n","        for i in range(bootstraps):\n","            # stratified sampling of positive and negative examples\n","            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n","            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n","\n","            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n","            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n","            score = metric_func(y_sample, pred_sample)\n","            statistics[c][i] = score\n","    return statistics\n","\n","def get_confidence_intervals(y,pred,class_labels):\n","    \n","    metric_dfs = {}\n","    for metric in ['AUC','Sensitivity','Specificity','Accuracy']:\n","        statistics = bootstrap_metric(y,pred,class_labels,metric)\n","        df = pd.DataFrame(columns=[\"Mean \"+metric+\" (CI 5%-95%)\"])\n","        for i in range(len(class_labels)):\n","            mean = statistics.mean(axis=1)[i]\n","            max_ = np.quantile(statistics, .95, axis=1)[i]\n","            min_ = np.quantile(statistics, .05, axis=1)[i]\n","            df.loc[class_labels[i]] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n","        metric_dfs[metric] = df\n","    return metric_dfs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682321963294,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"Z33SoIblLgRT"},"outputs":[],"source":["def plot_accuracy(tr_acc,val_acc):\n","    # Plot training & validation accuracy values\n","    plt.plot(tr_acc)\n","    plt.plot(val_acc)\n","    plt.title('Model accuracy',fontsize=10)\n","    plt.ylabel('Accuracy',fontsize=10)\n","    plt.xlabel('Epoch',fontsize=10)\n","    plt.tick_params(axis='both', which='major', labelsize=10)\n","    plt.legend(['Train', 'Validation'], loc='upper left',prop={'size': 10})\n","    plt.savefig('accuracy_plot.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682321963294,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"u9a5pumDLhyb"},"outputs":[],"source":["def plot_loss(tr_loss,val_loss):\n","    # Plot training & validation loss values\n","    plt.plot(tr_loss)\n","    plt.plot(val_loss)\n","    plt.title('Model loss',fontsize=10)\n","    plt.ylabel('Loss',fontsize=10)\n","    plt.xlabel('Epoch',fontsize=10)\n","    plt.tick_params(axis='both', which='major', labelsize=10)\n","    plt.legend(['Train', 'Validation'], loc='upper left',prop={'size': 10})\n","    plt.savefig('loss_plot.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682321963294,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"C3r8kX96LjNr"},"outputs":[],"source":["def get_roc_curve(gt, pred, target_names):\n","    for i in range(len(target_names)):\n","        curve_function = roc_curve\n","        auc_roc = roc_auc_score(gt[:, i], pred[:, i])\n","        label = str(target_names[i]) + \" AUC: %.3f \" % auc_roc\n","        xlabel = \"False positive rate\"\n","        ylabel = \"True positive rate\"\n","        a, b, _ = curve_function(gt[:, i], pred[:, i])\n","        plt.figure(1, figsize=(7, 7))\n","        plt.plot([0, 1], [0, 1], 'k--')\n","        plt.plot(a, b, label=label)\n","        plt.xlabel(xlabel)\n","        plt.ylabel(ylabel)\n","        plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n","                       fancybox=True, ncol=1)\n","        plt.savefig('ROC_Curve.png')\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682321963294,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"b1NwF6MkLkRr"},"outputs":[],"source":["def get_PR_curve(gt, pred, target_names):\n","    for i in range(len(target_names)):\n","        precision, recall, _ = precision_recall_curve(gt[:, i], pred[:, i])\n","        average_precision = average_precision_score(gt[:, i], pred[:, i])\n","        label = str(target_names[i]) + \" Avg.: %.3f \" % average_precision\n","        plt.figure(1, figsize=(7, 7))\n","        plt.step(recall, precision, where='post', label=label)\n","        plt.xlabel('Recall')\n","        plt.ylabel('Precision')\n","        plt.ylim([0.0, 1.05])\n","        plt.xlim([0.0, 1.0])\n","        plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n","                       fancybox=True, ncol=1)\n","        plt.savefig('Precision_and_Recall_curve.png')\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682321963294,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"w8cp5tKdLl3N"},"outputs":[],"source":["def plot_confusion_matrix(y_true,y_pred,class_labels):\n","    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n","    cm_sum = np.sum(cm, axis=1, keepdims=True)\n","    cm_perc = cm / cm_sum.astype(float) * 100\n","    annot = np.empty_like(cm).astype(str)\n","    nrows, ncols = cm.shape\n","    for i in range(nrows):\n","        for j in range(ncols):\n","            c = cm[i, j]\n","            p = cm_perc[i, j]\n","            if i == j:\n","                s = cm_sum[i]\n","                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n","            elif c == 0:\n","                annot[i, j] = ''\n","            else:\n","                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n","    cm = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n","    cm.index.name = 'Actual'\n","    cm.columns.name = 'Predicted'\n","    fig, ax = plt.subplots(figsize=(60,60))\n","    sns.set(font_scale=3.0) # Adjust to fit\n","    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n","    ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n","    ax.xaxis.set_ticklabels(class_labels)\n","    ax.yaxis.set_ticklabels(class_labels)\n","    fig.savefig('Confusion_Matrix.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682321963295,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"-h26x0szLzvN"},"outputs":[],"source":["#04_03_Errorbar.ipynb\n","def plot_perf_metrics_errbars(y,pred,class_labels):\n","    metric_dfs = get_confidence_intervals(y,pred,class_labels)\n","    metrics = metric_dfs.keys()\n","    fig,axs = plt.subplots(len(metrics),1,sharey=True)\n","    for i in range(len(metrics)):\n","        ci = metric_dfs[metric][['Mean '+metrics[i]+' (CI 5%-95%)']].values\n","        ci_mean,ci_ints = np.array([c[0].split(' ') for c in ci]).T\n","        ci_mean = ci_mean.astype(float)\n","        ci_min,ci_max = np.array([ci_ints.strip('()').split('-')]).astype(float)\n","        ci_err = (ci_max-ci_min)/2\n","        \n","        axs[i].errorbar(class_labels,ci_mean,yerr=ci_err,capsize=5,fmt='dk')\n","        axs[i].set_ylabel(metrics[i])\n","    fig.savefig('Performance_Metrics_95percentCI.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682321963295,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"KhYR8ADPL1kx"},"outputs":[],"source":["class Cutout(nn.Module):\n","    def __init__(self, seed = 0):\n","        self.seed = seed\n","        \n","    def get_start_index(self,L):\n","        return np.random.randint(L)\n","    \n","    def __call__(self,frame):\n","        channels, h, w = frame.shape\n","        #print(frames.shape)\n","        size = h//4\n","        n_squares = np.random.randint(1,3,1)[0]\n","        new_image = frame\n","        for _ in range(n_squares):\n","            y = np.clip(self.get_start_index(h), size // 2, h - size//2)\n","            x = np.clip(self.get_start_index(w), size // 2, w - size//2)\n","            \n","            y1 = np.clip(y - size // 2, 0, h)\n","            y2 = np.clip(y + size // 2, 0, h)\n","            x1 = np.clip(x - size // 2, 0, w)\n","            x2 = np.clip(x + size // 2, 0, w)\n","            new_image[:, y1:y2,x1:x2] = 0\n","        return new_image"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682321963295,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"2e3OXHe6L2iZ"},"outputs":[],"source":["class AddGaussianNoise(nn.Module):\n","    def __init__(self, mean=0., std=1.):\n","        self.std = std\n","        self.mean = mean\n","        \n","    def __call__(self, tensor):\n","        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n","    \n","    def __repr__(self):\n","        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1765,"status":"ok","timestamp":1682321965051,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"C3V_nXt8L361","outputId":"6ab9e4f6-c1b9-4a4d-e2c6-797174966e7d"},"outputs":[],"source":["MEAN = np.mean(images[:40000]/255.0,axis=(0,2,3),keepdims=True)\n","STD = np.std(images[:40000]/255.0,axis=(0,2,3),keepdims=True)\n","\n","print(MEAN, STD)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682321965052,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"jzkbjDO5MCFI"},"outputs":[],"source":["class C10DataGen(Dataset):\n","    def __init__(self,phase,imgarr,s = 0.5):\n","        self.phase = phase\n","        self.imgarr = imgarr\n","        self.s = s\n","        self.transforms = transforms.Compose([transforms.RandomHorizontalFlip(0.5),\n","                                              transforms.RandomResizedCrop(32,(0.8,1.0)),\n","                                              transforms.Compose([transforms.RandomApply([transforms.ColorJitter(0.8*self.s, \n","                                                                                                                 0.8*self.s, \n","                                                                                                                 0.8*self.s, \n","                                                                                                                 0.2*self.s)], p = 0.8),\n","                                                                  transforms.RandomGrayscale(p=0.2)\n","                                                                 ])])\n","\n","    def __len__(self):\n","        return self.imgarr.shape[0]\n","\n","    def __getitem__(self,idx):\n","        \n","        x = self.imgarr[idx] \n","        #print(x.shape)\n","        x = x.astype(np.float32)/255.0\n","\n","        x1 = self.augment(torch.from_numpy(x))\n","        x2 = self.augment(torch.from_numpy(x))\n","        \n","        x1 = self.preprocess(x1)\n","        x2 = self.preprocess(x2)\n","        \n","        return x1, x2\n","\n","    #shuffles the dataset at the end of each epoch\n","    def on_epoch_end(self):\n","        self.imgarr = self.imgarr[random.sample(population = list(range(self.__len__())),k = self.__len__())]\n","\n","    def preprocess(self,frame):\n","        frame = (frame-MEAN)/STD\n","        return frame\n","    \n","    #applies randomly selected augmentations to each clip (same for each frame in the clip)\n","    def augment(self, frame, transformations = None):\n","        \n","        if self.phase == 'train':\n","            frame = self.transforms(frame)\n","        else:\n","            return frame\n","        \n","        return frame"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682321965052,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"C_u10iQpMEJf"},"outputs":[],"source":["dg = C10DataGen('train',images) #train_df))\n","dl = DataLoader(dg,batch_size = 16)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9751,"status":"ok","timestamp":1682321974800,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"VDjKXCulMF0j","outputId":"6faca6bf-838d-4281-f774-6e65d54af4d9"},"outputs":[],"source":["#MEAN = np.array([0.485, 0.456, 0.406]).reshape((1,1,3))\n","#STD = np.array([0.229, 0.224, 0.25]).reshape((1,1,3))\n","fig,axs = plt.subplots(16,2,figsize=(16,128))\n","row = 0\n","col = 0\n","for step,(x1,x2) in enumerate(dl):\n","    print(x1.shape,x2.shape)\n","    for i in range(16):\n","        #print(x1[i]*STD + MEAN)\n","        #print(x2[i]*STD + MEAN)\n","        axs[i,0].imshow((x1[i]*STD + MEAN).squeeze().permute(1,2,0).numpy())\n","        axs[i,1].imshow((x2[i]*STD + MEAN).squeeze().permute(1,2,0).numpy())\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682321974801,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"MNEgAGlZMOWR"},"outputs":[],"source":["set_seed(16)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682321974801,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"1-4IhAl5MR1y"},"outputs":[],"source":["class Identity(nn.Module):\n","    def __init__(self):\n","        super(Identity, self).__init__()\n","\n","    def forward(self, x):\n","        return x\n","\n","class LinearLayer(nn.Module):\n","    def __init__(self,\n","                 in_features,\n","                 out_features,\n","                 use_bias = True,\n","                 use_bn = False,\n","                 **kwargs):\n","        super(LinearLayer, self).__init__(**kwargs)\n","\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.use_bias = use_bias\n","        self.use_bn = use_bn\n","        \n","        self.linear = nn.Linear(self.in_features, \n","                                self.out_features, \n","                                bias = self.use_bias and not self.use_bn)\n","        if self.use_bn:\n","             self.bn = nn.BatchNorm1d(self.out_features)\n","\n","    def forward(self,x):\n","        x = self.linear(x)\n","        if self.use_bn:\n","            x = self.bn(x)\n","        return x\n","\n","class ProjectionHead(nn.Module):\n","    def __init__(self,\n","                 in_features,\n","                 hidden_features,\n","                 out_features,\n","                 head_type = 'nonlinear',\n","                 **kwargs):\n","        super(ProjectionHead,self).__init__(**kwargs)\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.hidden_features = hidden_features\n","        self.head_type = head_type\n","\n","        if self.head_type == 'linear':\n","            self.layers = LinearLayer(self.in_features,self.out_features,False, True)\n","        elif self.head_type == 'nonlinear':\n","            self.layers = nn.Sequential(\n","                LinearLayer(self.in_features,self.hidden_features,True, True),\n","                nn.ReLU(),\n","                LinearLayer(self.hidden_features,self.out_features,False,True))\n","        \n","    def forward(self,x):\n","        x = self.layers(x)\n","        return x\n","\n","class PreModel(nn.Module):\n","    def __init__(self,base_model,base_out_layer):\n","        super().__init__()\n","        self.base_model = base_model\n","        self.base_out_layer = base_out_layer\n","        \n","        #PRETRAINED MODEL\n","        self.pretrained = models.resnet50(pretrained=True)\n","        \n","        self.pretrained.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n","        self.pretrained.maxpool = Identity()\n","        \n","        self.pretrained.fc = Identity()\n","        \n","        for p in self.pretrained.parameters():\n","            p.requires_grad = True\n","        \n","        self.projector = ProjectionHead(2048, 2048, 128)\n","    \n","\n","    def forward(self,x):\n","        out = self.pretrained(x)\n","        xp = self.projector(torch.squeeze(out))\n","        return xp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":554,"status":"ok","timestamp":1682321975348,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"fW0qpqttMT-n","outputId":"1aad49d5-3f2b-4548-cc83-f1663160adfc"},"outputs":[],"source":["device = 'cpu'\n","model = PreModel('resnet18','avgpool')\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682321975349,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"0eXD4nXNMamM","outputId":"b38fadc0-8a8c-4fcb-ef22-8edf8832e417"},"outputs":[],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682321975349,"user":{"displayName":"Fæster (Fæster)","userId":"06504261986022353900"},"user_tz":-120},"id":"jLZ2eaFcMb4I"},"outputs":[],"source":["x = np.random.random((32,3,224,224))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsHjBHxWMd9l"},"outputs":[],"source":["out = model(torch.tensor(x, device = 'cpu', dtype = torch.float32))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out[0].shape, out[1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_optimizer(arg_optimizer, model, batch_size):\n","\n","    scheduler = None\n","    if arg_optimizer == \"Adam\":\n","        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)  # TODO: LARS\n","    elif arg_optimizer == \"LARS\":\n","        # optimized using LARS with linear learning rate scaling\n","        # (i.e. LearningRate = 0.3 × BatchSize/256) and weight decay of 10−6.\n","        learning_rate = 0.3 * batch_size / 256\n","        optimizer = LARS(\n","            [params for params in model.parameters() if params.requires_grad],\n","            lr=0.1,\n","            weight_decay=1e-6,\n","            exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n","        )\n","\n","        # \"decay the learning rate with the cosine decay schedule without restarts\"\n","        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","            optimizer, epochs, eta_min=0, last_epoch=-1\n","        )\n","    else:\n","        raise NotImplementedError\n","\n","    return optimizer, scheduler\n","\n","def save_model(model, optimizer, scheduler, current_epoch, name):\n","    out = os.path.join('/content/saved_models/',name.format(current_epoch))\n","\n","    torch.save({'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'scheduler_state_dict':scheduler.state_dict()}, out)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!mkdir content/saved_models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SimCLR_Loss(nn.Module):\n","    def __init__(self, batch_size, temperature):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.temperature = temperature\n","\n","        self.mask = self.mask_correlated_samples(batch_size)\n","        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","        self.similarity_f = nn.CosineSimilarity(dim=2)\n","        \n","        self.tot_neg = 0\n","\n","    def mask_correlated_samples(self, batch_size):\n","        N = 2 * batch_size\n","        mask = torch.ones((N, N), dtype=bool)\n","        mask = mask.fill_diagonal_(0)\n","        \n","        for i in range(batch_size):\n","            mask[i, batch_size + i] = 0\n","            mask[batch_size + i, i] = 0\n","            \n","        return mask\n","\n","    def forward(self, z_i, z_j):\n","        \"\"\"\n","        We do not sample negative examples explicitly.\n","        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N − 1) augmented examples within a minibatch as negative examples.\n","        \"\"\"\n","        N = 2 * self.batch_size #* self.world_size\n","        \n","        #z_i_ = z_i / torch.sqrt(torch.sum(torch.square(z_i),dim = 1, keepdim = True))\n","        #z_j_ = z_j / torch.sqrt(torch.sum(torch.square(z_j),dim = 1, keepdim = True))\n","\n","        z = torch.cat((z_i, z_j), dim=0)\n","\n","        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n","        \n","        #print(sim.shape)\n","\n","        sim_i_j = torch.diag(sim, self.batch_size)\n","        sim_j_i = torch.diag(sim, -self.batch_size)\n","        \n","        \n","        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n","        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n","        negative_samples = sim[self.mask].reshape(N, -1)\n","        \n","        \n","        #SIMCLR\n","        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n","        #labels was torch.zeros(N)\n","        logits = torch.cat((positive_samples, negative_samples), dim=1)\n","        loss = self.criterion(logits, labels)\n","        loss /= N\n","        \n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.optim.optimizer import Optimizer, required\n","import re\n","\n","EETA_DEFAULT = 0.001\n","\n","\n","class LARS(Optimizer):\n","    \"\"\"\n","    Layer-wise Adaptive Rate Scaling for large batch training.\n","    Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n","    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        params,\n","        lr=required,\n","        momentum=0.9,\n","        use_nesterov=False,\n","        weight_decay=0.0,\n","        exclude_from_weight_decay=None,\n","        exclude_from_layer_adaptation=None,\n","        classic_momentum=True,\n","        eeta=EETA_DEFAULT,\n","    ):\n","        \"\"\"Constructs a LARSOptimizer.\n","        Args:\n","        lr: A `float` for learning rate.\n","        momentum: A `float` for momentum.\n","        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n","        weight_decay: A `float` for weight decay.\n","        exclude_from_weight_decay: A list of `string` for variable screening, if\n","            any of the string appears in a variable's name, the variable will be\n","            excluded for computing weight decay. For example, one could specify\n","            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n","            from weight decay.\n","        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n","            for layer adaptation. If it is None, it will be defaulted the same as\n","            exclude_from_weight_decay.\n","        classic_momentum: A `boolean` for whether to use classic (or popular)\n","            momentum. The learning rate is applied during momeuntum update in\n","            classic momentum, but after momentum for popular momentum.\n","        eeta: A `float` for scaling of learning rate when computing trust ratio.\n","        name: The name for the scope.\n","        \"\"\"\n","\n","        self.epoch = 0\n","        defaults = dict(\n","            lr=lr,\n","            momentum=momentum,\n","            use_nesterov=use_nesterov,\n","            weight_decay=weight_decay,\n","            exclude_from_weight_decay=exclude_from_weight_decay,\n","            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n","            classic_momentum=classic_momentum,\n","            eeta=eeta,\n","        )\n","\n","        super(LARS, self).__init__(params, defaults)\n","        self.lr = lr\n","        self.momentum = momentum\n","        self.weight_decay = weight_decay\n","        self.use_nesterov = use_nesterov\n","        self.classic_momentum = classic_momentum\n","        self.eeta = eeta\n","        self.exclude_from_weight_decay = exclude_from_weight_decay\n","        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n","        # arg is None.\n","        if exclude_from_layer_adaptation:\n","            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n","        else:\n","            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n","\n","    def step(self, epoch=None, closure=None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        if epoch is None:\n","            epoch = self.epoch\n","            self.epoch += 1\n","\n","        for group in self.param_groups:\n","            weight_decay = group[\"weight_decay\"]\n","            momentum = group[\"momentum\"]\n","            eeta = group[\"eeta\"]\n","            lr = group[\"lr\"]\n","\n","            for p in group[\"params\"]:\n","                if p.grad is None:\n","                    continue\n","\n","                param = p.data\n","                grad = p.grad.data\n","\n","                param_state = self.state[p]\n","\n","                # TODO: get param names\n","                # if self._use_weight_decay(param_name):\n","                grad += self.weight_decay * param\n","\n","                if self.classic_momentum:\n","                    trust_ratio = 1.0\n","\n","                    # TODO: get param names\n","                    # if self._do_layer_adaptation(param_name):\n","                    w_norm = torch.norm(param)\n","                    g_norm = torch.norm(grad)\n","\n","                    device = g_norm.get_device()\n","                    trust_ratio = torch.where(\n","                        w_norm.gt(0),\n","                        torch.where(\n","                            g_norm.gt(0),\n","                            (self.eeta * w_norm / g_norm),\n","                            torch.Tensor([1.0]).to(device),\n","                        ),\n","                        torch.Tensor([1.0]).to(device),\n","                    ).item()\n","\n","                    scaled_lr = lr * trust_ratio\n","                    if \"momentum_buffer\" not in param_state:\n","                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n","                            p.data\n","                        )\n","                    else:\n","                        next_v = param_state[\"momentum_buffer\"]\n","\n","                    next_v.mul_(momentum).add_(scaled_lr, grad)\n","                    if self.use_nesterov:\n","                        update = (self.momentum * next_v) + (scaled_lr * grad)\n","                    else:\n","                        update = next_v\n","\n","                    p.data.add_(-update)\n","                else:\n","                    raise NotImplementedError\n","\n","        return loss\n","\n","    def _use_weight_decay(self, param_name):\n","        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n","        if not self.weight_decay:\n","            return False\n","        if self.exclude_from_weight_decay:\n","            for r in self.exclude_from_weight_decay:\n","                if re.search(r, param_name) is not None:\n","                    return False\n","        return True\n","\n","    def _do_layer_adaptation(self, param_name):\n","        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n","        if self.exclude_from_layer_adaptation:\n","            for r in self.exclude_from_layer_adaptation:\n","                if re.search(r, param_name) is not None:\n","                    return False\n","        return True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimizer = LARS(\n","    [params for params in model.parameters() if params.requires_grad],\n","    lr=0.2,\n","    weight_decay=1e-6,\n","    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",")\n","\n","# \"decay the learning rate with the cosine decay schedule without restarts\"\n","warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n","mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion = SimCLR_Loss(batch_size = 128, temperature = 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trimages = images[:40000]\n","valimages = images[40000:]\n","trlabels = labels[:40000]\n","vallabels = labels[40000:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dg = C10DataGen('train',trimages)#train_df)\n","dl = DataLoader(dg,batch_size = 128,drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vdg = C10DataGen('valid',valimages)#_df)\n","vdl = DataLoader(vdg,batch_size = 128,drop_last=True)\n","print(len(vdl))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.hist(vallabels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nr = 0\n","global_step = 0\n","current_epoch = 0\n","epochs = 100"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_features(model, num_classes, num_feats, batch_size, val_df = None):\n","    preds = np.array([]).reshape((0,1))\n","    gt = np.array([]).reshape((0,1))\n","    feats = np.array([]).reshape((0,num_feats))\n","    model.eval()\n","    with torch.no_grad():\n","        for x1,x2 in vdl:\n","            x1 = x1.squeeze().to(device = device, dtype = torch.float)#.view((-1,3,224,224))\n","            #y = y.to(device = 'cuda:0')#.view((-1,1))\n","            out = model(x1)\n","            out = out.cpu().data.numpy()#.reshape((1,-1))\n","            feats = np.append(feats,out,axis = 0)\n","    \n","    tsne = TSNE(n_components = 2, perplexity = 50)\n","    x_feats = tsne.fit_transform(feats)\n","    #plt.scatter(x_feats[:,1],x_feats[:,0])\n","    num_samples = int(batch_size*(valimages.shape[0]//batch_size))#(len(val_df)\n","    \n","    for i in range(num_classes):\n","        #plt.scatter(x_feats[val_df['class'].iloc[:num_samples].values==i,1],x_feats[val_df['class'].iloc[:num_samples].values==i,0])\n","        plt.scatter(x_feats[vallabels[:num_samples]==i,1],x_feats[vallabels[:num_samples]==i,0])\n","    \n","    plt.legend([str(i) for i in range(num_classes)])\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_features(model.pretrained, 10, 2048, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(train_loader, model, criterion, optimizer):\n","    loss_epoch = 0\n","    \n","    for step, (x_i, x_j) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        x_i = x_i.squeeze().to('cuda:0').float()\n","        x_j = x_j.squeeze().to('cuda:0').float()\n","\n","        # positive pair, with encoding\n","        z_i = model(x_i)\n","        z_j = model(x_j)\n","\n","        loss = criterion(z_i, z_j)\n","        loss.backward()\n","\n","        optimizer.step()\n","        \n","        if nr == 0 and step % 50 == 0:\n","            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {round(loss.item(), 5)}\")\n","\n","        loss_epoch += loss.item()\n","    return loss_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def valid(valid_loader, model, criterion):\n","    loss_epoch = 0\n","    for step, (x_i, x_j) in enumerate(valid_loader):\n","        \n","        x_i = x_i.squeeze().to('cuda:0').float()\n","        x_j = x_j.squeeze().to('cuda:0').float()\n","\n","        # positive pair, with encoding\n","        z_i = model(x_i)\n","        z_j = model(x_j)\n","\n","        loss = criterion(z_i, z_j)\n","        \n","        if nr == 0 and step % 50 == 0:\n","            print(f\"Step [{step}/{len(valid_loader)}]\\t Loss: {round(loss.item(),5)}\")\n","\n","        loss_epoch += loss.item()\n","    return loss_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tr_loss = []\n","val_loss = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(1):\n","        \n","    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n","    stime = time.time()\n","    model = model.to('cuda')\n","    model.train()\n","    tr_loss_epoch = train(dl, model, criterion, optimizer)\n","\n","    if nr == 0 and epoch < 10:\n","        warmupscheduler.step()\n","    if nr == 0 and epoch >= 10:\n","        mainscheduler.step()\n","    \n","    lr = optimizer.param_groups[0][\"lr\"]\n","\n","    if nr == 0 and (epoch+1) % 50 == 0:\n","        save_model(model, optimizer, mainscheduler, current_epoch,\"SimCLR_CIFAR10_RN50_P128_LR0P2_LWup10_Cos500_T0p5_B128_checkpoint_{}_260621.pt\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        val_loss_epoch = valid(vdl, model, criterion)\n","\n","    if nr == 0:\n","        \n","        tr_loss.append(tr_loss_epoch / len(dl))\n","        \n","        val_loss.append(val_loss_epoch / len(vdl))\n","        \n","        print(\n","            f\"Epoch [{epoch}/{epochs}]\\t Training Loss: {tr_loss_epoch / len(dl)}\\t lr: {round(lr, 5)}\"\n","        )\n","        print(\n","            f\"Epoch [{epoch}/{epochs}]\\t Validation Loss: {val_loss_epoch / len(vdl)}\\t lr: {round(lr, 5)}\"\n","        )\n","        current_epoch += 1\n","\n","    dg.on_epoch_end()\n","\n","    time_taken = (time.time()-stime)/60\n","    print(f\"Epoch [{epoch}/{epochs}]\\t Time Taken: {time_taken} minutes\")\n","\n","    if (epoch+1)%10==0:\n","        plot_features(model.pretrained, 10, 2048, 128) #, valimages)\n","\n","## end training\n","save_model(model, optimizer, mainscheduler, current_epoch, \"SimCLR_CIFAR10_RN50_P128_LR0P2_LWup10_Cos500_T0p5_B128_checkpoint_{}_260621.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.unique(vallabels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_model(model, optimizer, scheduler, current_epoch, \"SimCLR_IMgNet200_RN50_P512_LR0P5_B128_checkpoint_{}_140621.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(tr_loss,'b-')\n","plt.plot(val_loss,'r-')\n","plt.legend(['t','v'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DSModel(nn.Module):\n","    def __init__(self,premodel,num_classes):\n","        super().__init__()\n","        \n","        self.premodel = premodel\n","        \n","        self.num_classes = num_classes\n","\n","        #TAKING OUTPUT FROM AN INTERMEDITATE LAYER\n","        #PREPRAING THE TRUNCATED MODEL\n","        \n","        for p in self.premodel.parameters():\n","            p.requires_grad = False\n","            \n","        for p in self.premodel.projector.parameters():\n","            p.requires_grad = False\n","        \n","        \n","        self.lastlayer = nn.Linear(2048,self.num_classes)\n","        \n","\n","        \n","    def forward(self,x):\n","        out = self.premodel.pretrained(x)\n","        \n","        out = self.lastlayer(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["set_seed(16)\n","dsmodel = DSModel(model, 10).to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NUM_FRAMES = 16\n","NUM_CLASSES = 10"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DSDataGen(Dataset):\n","    def __init__(self, phase, imgarr,labels,num_classes=NUM_CLASSES):\n","        \n","        self.phase = phase\n","        self.num_classes = num_classes\n","        self.imgarr = imgarr\n","        self.labels = labels\n","        \n","        self.indices = list(range(self.imgarr.shape[0]))\n","        \n","        self.randomcrop = transforms.RandomResizedCrop(32,(0.8,1.0))\n","    \n","    def __len__(self):\n","        return self.imgarr.shape[0]\n","    \n","    def __getitem__(self,idx):\n","        \n","        #DECLARE VARIABLES\n","        \n","        x = self.imgarr[idx]\n","        \n","        img = torch.from_numpy(x).float()\n","            \n","        #GET CLIP FRAMES\n","        #for i in range(4):\n","        label = self.labels[idx]\n","\n","        #AUGMENT FRAMES\n","        if self.phase == 'train':\n","            img  = self.randomcrop(img)\n","\n","        img = self.preprocess(img)\n","        \n","        return img, label\n","    \n","    def on_epoch_end(self):\n","        idx = random.sample(population = list(range(self.__len__())),k = self.__len__())\n","        self.imgarr = self.imgarr[idx]\n","        self.labels = self.labels[idx]\n","        \n","    def preprocess(self,frame):\n","        frame = frame / 255.0\n","        frame = (frame-MEAN)/STD\n","        \n","        return frame"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dg = DSDataGen('train', trimages,trlabels,\n","               batch_size=1, num_classes=NUM_CLASSES)\n","\n","dl = DataLoader(dg,batch_size = 32, drop_last = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","vdg = DSDataGen('valid', valimages, vallabels,\n","               batch_size=1,num_classes=NUM_CLASSES)\n","\n","vdl = DataLoader(vdg,batch_size = 32, drop_last = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dsoptimizer = torch.optim.SGD([params for params in dsmodel.parameters() if params.requires_grad],lr = 0.01, momentum = 0.9)\n","\n","#dsoptimizer = torch.optim.Adam([params for params in dsmodel.parameters() if params.requires_grad],lr=1e-5)\n","\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(dsoptimizer, step_size=1, gamma=0.98, last_epoch=-1, verbose = True) #CosineAnnealingWarmRestarts(dsoptimizer,5,eta_min = 1e-6,last_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tr_ep_loss = []\n","tr_ep_acc = []\n","tr_ep_auc = []\n","val_ep_loss = []\n","val_ep_acc = []\n","val_ep_auc = []\n","\n","min_val_loss = 100.0\n","\n","batch_size = 1\n","EPOCHS = 10\n","num_cl = 10\n","accumulation_steps = 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(20):\n","    \n","    stime = time.time()\n","    print(\"=============== Epoch : %3d ===============\"%(epoch+1))\n","    \n","    loss_sublist = np.array([])\n","    acc_sublist = np.array([])\n","    \n","    #iter_num = 0\n","    dsmodel.train()\n","    \n","    dsoptimizer.zero_grad()\n","    \n","    for x,y in dl:\n","        x = x.squeeze().to(device = 'cuda:0', dtype = torch.float)\n","        y = y.to(device = 'cuda:0')\n","        \n","        z = dsmodel(x)\n","        \n","        dsoptimizer.zero_grad()\n","        \n","        tr_loss = loss_fn(z,y) #/accumulation_steps #y.to(dtype=torch.float), z)\n","        tr_loss.backward()\n","\n","        preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n","        \n","        #if (iter_num+1)%accumulation_steps==0:\n","        dsoptimizer.step()\n","        #dsoptimizer.zero_grad()\n","        \n","        loss_sublist = np.append(loss_sublist, tr_loss.cpu().data)\n","        acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n","        \n","        #iter_num+=1\n","    \n","    print('ESTIMATING TRAINING METRICS.............')\n","    \n","    print('TRAINING BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist)*accumulation_steps)\n","    print('TRAINING BINARY ACCURACY: ',np.mean(acc_sublist))\n","    #print('TRAINING AUC SCORE: ',roc_auc_score(gt,preds))\n","    \n","    tr_ep_loss.append(np.mean(loss_sublist))\n","    tr_ep_acc.append(np.mean(acc_sublist))\n","    \n","    #tr_ep_auc.append(roc_auc_score(gt, preds))\n","    \n","    \n","    \n","    print('ESTIMATING VALIDATION METRICS.............')\n","    \n","    dsmodel.eval()\n","    \n","    loss_sublist = np.array([])\n","    acc_sublist = np.array([])\n","    \n","    with torch.no_grad():\n","        for x,y in vdl:\n","            x = x.squeeze().to(device = 'cuda:0', dtype = torch.float)\n","            y = y.to(device = 'cuda:0')\n","            z = dsmodel(x)\n","\n","            val_loss = loss_fn(z,y)\n","\n","            preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n","\n","            loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n","            acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n","            \n","\n","    print('VALIDATION BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n","    print('VALIDATION BINARY ACCURACY: ',np.mean(acc_sublist))\n","    #print('VALIDATION AUC SCORE: ',roc_auc_score(gt, preds))\n","    \n","    val_ep_loss.append(np.mean(loss_sublist))\n","    val_ep_acc.append(np.mean(acc_sublist))\n","    \n","    #val_ep_auc.append(roc_auc_score(gt, preds))\n","    \n","    lr_scheduler.step()\n","    \n","    dg.on_epoch_end()\n","    \n","    if np.mean(loss_sublist) <= min_val_loss:\n","        min_val_loss = np.mean(loss_sublist) \n","        print('Saving model...')\n","        torch.save({'model_state_dict': dsmodel.state_dict(),\n","                'optimizer_state_dict': dsoptimizer.state_dict()}, \n","               '/content/saved_models/cifar10_rn50_p128_sgd0p01_decay0p98_all_lincls_300621.pt')\n","    \n","    print(\"Time Taken : %.2f minutes\"%((time.time()-stime)/60.0))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plt.plot([t for t in tr_ep_acc])\n","plt.plot([t for t in val_ep_acc])\n","plt.legend(['train','valid'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(tr_ep_loss)\n","plt.plot(val_ep_loss)\n","plt.legend(['train','valid'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot([t for t in tr_ep_auc])\n","plt.plot([t for t in val_ep_auc])\n","plt.legend(['train','valid'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdg = DSDataGen('test', testimages, testlabels, num_classes=10)\n","\n","tdl = DataLoader(tdg, batch_size = 32, drop_last = True)\n","\n","dsmodel.eval()\n","    \n","loss_sublist = np.array([])\n","acc_sublist = np.array([])\n","\n","with torch.no_grad():\n","    for x,y in vdl:\n","        x = x.squeeze().to(device = 'cuda:0', dtype = torch.float)\n","        y = y.to(device = 'cuda:0')\n","        z = dsmodel(x)\n","\n","        val_loss = loss_fn(z,y)\n","\n","        preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n","\n","        loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n","        acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n","\n","print('TEST BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n","print('TEST BINARY ACCURACY: ',np.mean(acc_sublist))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMRIbf/bmZfx7D0nlyqMk1m","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
