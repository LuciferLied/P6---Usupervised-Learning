{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Util\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from thop import profile, clever_format\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Pair(CIFAR10):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            pos_1 = self.transform(img)\n",
    "            pos_2 = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return pos_1, pos_2, target\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.encoder = []\n",
    "        for name, module in resnet18().named_children():\n",
    "            if name == 'conv1':\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
    "                self.encoder.append(module)\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(*self.encoder)\n",
    "        # projection head\n",
    "        self.g = nn.Sequential(nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512),\n",
    "                               nn.ReLU(inplace=True), nn.Linear(512, feature_dim, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.g(feature)\n",
    "        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "epochs = 1\n",
    "feature_dim = 128\n",
    "temperature = 0.5\n",
    "k = 200\n",
    "\n",
    "# train for one epoch to learn unique features\n",
    "def train(net, data_loader, train_optimizer):\n",
    "    net.train()\n",
    "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
    "    for pos_1, pos_2, target in train_bar:\n",
    "        pos_1, pos_2 = pos_1.to(device,non_blocking=True), pos_2.to(device,non_blocking=True)\n",
    "        feature_1, out_1 = net(pos_1)\n",
    "        feature_2, out_2 = net(pos_2)\n",
    "        print('out1',out_1.shape)\n",
    "        print('out2',out_2.shape)\n",
    "        # [2*B, D]\n",
    "        out = torch.cat([out_1, out_2], dim=0)\n",
    "        print('out12',out.shape)\n",
    "        # [2*B, 2*B]\n",
    "        sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n",
    "        print('sim_matrix',sim_matrix.shape)\n",
    "        mask = (torch.ones_like(sim_matrix) - torch.eye(2 * batch_size, device=sim_matrix.device)).bool()\n",
    "        print('mask',mask.shape)\n",
    "        # [2*B, 2*B-1]\n",
    "        sim_matrix = sim_matrix.masked_select(mask).view(2 * batch_size, -1)\n",
    "        print('sim_matrix2',sim_matrix.shape)\n",
    "        # compute loss\n",
    "        pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "        print('pos_sim',pos_sim.shape)\n",
    "        # [2*B]\n",
    "        pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
    "        print('pos_sim2',pos_sim.shape)\n",
    "        loss = (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()\n",
    "        print('loss',loss.shape)\n",
    "        train_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_optimizer.step()\n",
    "\n",
    "        total_num += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, total_loss / total_num))\n",
    "    return total_loss / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for one epoch, use weighted knn to find the most similar images' label to assign the test image\n",
    "def test(net, memory_data_loader, test_data_loader):\n",
    "    net.eval()\n",
    "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
    "    with torch.no_grad():\n",
    "        # generate feature bank\n",
    "        for data, _, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
    "            feature, out = net(data.to(device,non_blocking=True))\n",
    "            feature_bank.append(feature)\n",
    "        # [D, N]\n",
    "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
    "        # [N]\n",
    "        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n",
    "        # loop test data to predict the label by weighted knn search\n",
    "        test_bar = tqdm(test_data_loader)\n",
    "        for data, _, target in test_bar:\n",
    "            data, target = data.to(device,non_blocking=True), target.to(device,non_blocking=True)\n",
    "            feature, out = net(data)\n",
    "\n",
    "            total_num += data.size(0)\n",
    "            # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
    "            sim_matrix = torch.mm(feature, feature_bank)\n",
    "            # [B, K]\n",
    "            sim_weight, sim_indices = sim_matrix.topk(k=k, dim=-1)\n",
    "            # [B, K]\n",
    "            sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)\n",
    "            sim_weight = (sim_weight / temperature).exp()\n",
    "\n",
    "            # counts for each class\n",
    "            one_hot_label = torch.zeros(data.size(0) * k, c, device=sim_labels.device)\n",
    "            # [B*K, C]\n",
    "            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
    "            # weighted score ---> [B, C]\n",
    "            pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, c) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
    "\n",
    "            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
    "            total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "            total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%'\n",
    "                                     .format(epoch, epochs, total_top1 / total_num * 100, total_top5 / total_num * 100))\n",
    "            \n",
    "    return total_top1 / total_num * 100, total_top5 / total_num * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\snk\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 6 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# data prepare\n",
    "train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
    "\n",
    "memory_data = CIFAR10Pair(root='data', train=True, transform=test_transform, download=True)\n",
    "memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "test_data = CIFAR10Pair(root='data', train=False, transform=test_transform, download=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "# Model Params: 11.50M FLOPs: 558.21M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/390 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# model setup and optimizer config\n",
    "model = Model(feature_dim).to(device)\n",
    "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
    "flops, params = clever_format([flops, params])\n",
    "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "c = len(memory_data.classes)\n",
    "\n",
    "# training loop\n",
    "results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
    "save_name_pre = '{}_{}_{}_{}_{}'.format(feature_dim, temperature, k, batch_size, epochs)\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    results['train_loss'].append(train_loss)\n",
    "    test_acc_1, test_acc_5 = test(model, memory_loader, test_loader)\n",
    "    results['test_acc@1'].append(test_acc_1)\n",
    "    results['test_acc@5'].append(test_acc_5)\n",
    "    # save statistics\n",
    "    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
    "    data_frame.to_csv('results/{}_statistics.csv'.format(save_name_pre), index_label='epoch')\n",
    "    if test_acc_1 > best_acc:\n",
    "        best_acc = test_acc_1\n",
    "        torch.save(model.state_dict(), 'results/{}_model.pth'.format(save_name_pre))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
